{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data dimensionality and chance patterns that may return as significant</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore the potential for patterns to arise by chance in data as we increase the number of features in a dataset. We create what is essentially a null model by randomly generating a dataset and performing some significance tests (a <i>t</i>-test in this case) on the features to assess what chance can achieve in terms of significant differences between randomly assigned classes.\n",
    "\n",
    "We find that the proportion of significant features over many tests tends towards the alpha level of significance we are using; i.e., ~5% (1/20 or 0.05) of features will randomly contain a \"signal\" at the typical alpha level of significance ($p$ = 0.05), or, more elegantly, the proportion of significant features ($P$) of the total number of features equals the alpha level of significance ($\\alpha$):\n",
    "\n",
    "$$P = \\alpha$$\n",
    "\n",
    "The number of likely significant features ($F_{s}$) of the total number of features ($F$) is approximately:\n",
    "\n",
    "$$F_{s} = F \\cdot \\alpha$$\n",
    "\n",
    "If we have a dataset containing 20 features, 1 would be expected to test as significant by chance with an $\\alpha$ of 0.05: \n",
    "\n",
    "$$F_{s} = F \\cdot \\alpha = 20 \\cdot 0.05 = 1$$\n",
    "\n",
    "However, spectral datasets typically contain hundreds or thousands of features. For instance, a typical fingerprint region of an FTIR dataset contains ~400 features. Using an alpha level of significance of approximately 0.05, we might expect that:\n",
    "\n",
    "$$F_{s} = F \\cdot \\alpha = 400 \\cdot 0.05 = 20$$\n",
    "\n",
    "20 features may test as significant by chance.\n",
    "\n",
    "This is demostrated in the following code. Given this potential, it is important to consider whether machine learning algorithms are finding genuine, useful patterns in data because as we increase the dimensionality of our data (the number of features), we increase the potential for algorithms to find chance patterns. Ultimately we may be fooled into thinking that we have found genuine, predictable differences between our samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Loading libraries, assigning functions and setting plotting options</h4>\n",
    "call.do() is simply a switch of arguments from do.call() that allows it to be used easily in piped code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(parallel)\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "call.do <- function(args, what){do.call(what, args)}\n",
    "options(repr.plot.width = 5, repr.plot.height = 5, repr.plot.res = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Creating a set of data randomly drawn from a uniform distribution. It has 8192 observations and 1024 features. (Many features are typical of spectra.)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>8192</li>\n",
       "\t<li>1024</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 8192\n",
       "\\item 1024\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 8192\n",
       "2. 1024\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 8192 1024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nObservation <- 2^13\n",
    "nFeature <- 2^10\n",
    "set.seed(1138)\n",
    "randomData <- lapply(1:nObservation, function(i){runif(nFeature)}) %>% call.do(rbind)\n",
    "dim(randomData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Randomly assigning observations to one of two groups, a and b, and adding that to the randomly generated data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nGroup <- 2 # this script will only run with two groups as we as using a t-test as a test of significance\n",
    "classSize <- nObservation / nGroup\n",
    "randomGroup <- rep(letters[1:nGroup], each = classSize)\n",
    "randomData <- data.frame(randomData) %>% cbind(data.frame(group = randomGroup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Establishing a sequence of numbers to iterate. It is base 2 where each number is double the previous one in the sequence. We will use it to iterate over various numbers of randomly selected observations and features</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>4</li>\n",
       "\t<li>8</li>\n",
       "\t<li>16</li>\n",
       "\t<li>32</li>\n",
       "\t<li>64</li>\n",
       "\t<li>128</li>\n",
       "\t<li>256</li>\n",
       "\t<li>512</li>\n",
       "\t<li>1024</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 4\n",
       "\\item 8\n",
       "\\item 16\n",
       "\\item 32\n",
       "\\item 64\n",
       "\\item 128\n",
       "\\item 256\n",
       "\\item 512\n",
       "\\item 1024\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 4\n",
       "2. 8\n",
       "3. 16\n",
       "4. 32\n",
       "5. 64\n",
       "6. 128\n",
       "7. 256\n",
       "8. 512\n",
       "9. 1024\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]    4    8   16   32   64  128  256  512 1024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAH0CAMAAAD8CC+4AAAABlBMVEUAAAD///+l2Z/dAAAA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\nRgcyOpDRgf4BHF7FqcaLSjgAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(base2_seq <- 2^c(2:10))\n",
    "plot(base2_seq, type = \"b\", las = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Defining a couple of functions to randomly subset data in terms of observations and features. This will happen many times to demonstrate that the tendency of a significant result is a function of the alpha level of significance of the statistical test.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampleObs <- function(n){\n",
    "    randomData %>% \n",
    "    split(randomData$group) %>% \n",
    "    lapply(function(x){x[sample(nrow(x), n), ]})\n",
    "}\n",
    "\n",
    "subsampleData <- function(nObs, nFeatures){\n",
    "    subSample <- subsampleObs(nObs) %>% call.do(rbind)\n",
    "    groups <- subSample$group\n",
    "    subSample <- subSample %>% \n",
    "    group_by(group) %>% \n",
    "    sample(nFeatures) %>% \n",
    "    ungroup()\n",
    "    subSample$group <- groups\n",
    "    return(subSample)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>X59</th><th scope=col>X941</th><th scope=col>X183</th><th scope=col>X215</th><th scope=col>group</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.8931902   </td><td>0.70882586  </td><td>0.9823403461</td><td>0.17824324  </td><td>a           </td></tr>\n",
       "\t<tr><td>0.9500228   </td><td>0.43068695  </td><td>0.5122523878</td><td>0.07181787  </td><td>a           </td></tr>\n",
       "\t<tr><td>0.7391530   </td><td>0.40384400  </td><td>0.0005237537</td><td>0.25888528  </td><td>a           </td></tr>\n",
       "\t<tr><td>0.6889495   </td><td>0.01261479  </td><td>0.9112691120</td><td>0.95739781  </td><td>a           </td></tr>\n",
       "\t<tr><td>0.7287171   </td><td>0.29628283  </td><td>0.4031528893</td><td>0.17877259  </td><td>b           </td></tr>\n",
       "\t<tr><td>0.6039772   </td><td>0.78171662  </td><td>0.0655915937</td><td>0.21866393  </td><td>b           </td></tr>\n",
       "\t<tr><td>0.4640840   </td><td>0.29012794  </td><td>0.9081497837</td><td>0.90662285  </td><td>b           </td></tr>\n",
       "\t<tr><td>0.1467098   </td><td>0.89873815  </td><td>0.1047934261</td><td>0.54316011  </td><td>b           </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " X59 & X941 & X183 & X215 & group\\\\\n",
       "\\hline\n",
       "\t 0.8931902    & 0.70882586   & 0.9823403461 & 0.17824324   & a           \\\\\n",
       "\t 0.9500228    & 0.43068695   & 0.5122523878 & 0.07181787   & a           \\\\\n",
       "\t 0.7391530    & 0.40384400   & 0.0005237537 & 0.25888528   & a           \\\\\n",
       "\t 0.6889495    & 0.01261479   & 0.9112691120 & 0.95739781   & a           \\\\\n",
       "\t 0.7287171    & 0.29628283   & 0.4031528893 & 0.17877259   & b           \\\\\n",
       "\t 0.6039772    & 0.78171662   & 0.0655915937 & 0.21866393   & b           \\\\\n",
       "\t 0.4640840    & 0.29012794   & 0.9081497837 & 0.90662285   & b           \\\\\n",
       "\t 0.1467098    & 0.89873815   & 0.1047934261 & 0.54316011   & b           \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| X59 | X941 | X183 | X215 | group |\n",
       "|---|---|---|---|---|\n",
       "| 0.8931902    | 0.70882586   | 0.9823403461 | 0.17824324   | a            |\n",
       "| 0.9500228    | 0.43068695   | 0.5122523878 | 0.07181787   | a            |\n",
       "| 0.7391530    | 0.40384400   | 0.0005237537 | 0.25888528   | a            |\n",
       "| 0.6889495    | 0.01261479   | 0.9112691120 | 0.95739781   | a            |\n",
       "| 0.7287171    | 0.29628283   | 0.4031528893 | 0.17877259   | b            |\n",
       "| 0.6039772    | 0.78171662   | 0.0655915937 | 0.21866393   | b            |\n",
       "| 0.4640840    | 0.29012794   | 0.9081497837 | 0.90662285   | b            |\n",
       "| 0.1467098    | 0.89873815   | 0.1047934261 | 0.54316011   | b            |\n",
       "\n"
      ],
      "text/plain": [
       "  X59       X941       X183         X215       group\n",
       "1 0.8931902 0.70882586 0.9823403461 0.17824324 a    \n",
       "2 0.9500228 0.43068695 0.5122523878 0.07181787 a    \n",
       "3 0.7391530 0.40384400 0.0005237537 0.25888528 a    \n",
       "4 0.6889495 0.01261479 0.9112691120 0.95739781 a    \n",
       "5 0.7287171 0.29628283 0.4031528893 0.17877259 b    \n",
       "6 0.6039772 0.78171662 0.0655915937 0.21866393 b    \n",
       "7 0.4640840 0.29012794 0.9081497837 0.90662285 b    \n",
       "8 0.1467098 0.89873815 0.1047934261 0.54316011 b    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is the output it produces given its input parameters: \n",
    "# 4 randomly selected observations of each class and 4 randomly selected features\n",
    "# For each iteration, this subsampling will be performed 100 times and a t-test applied\n",
    "subsampleData(nObs = 4, nFeatures = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Generating a table of values to iterate over (# of observations, # of features and p-value we are taking as significant and we will store the percentage of significantly different features in the column</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>8</li>\n",
       "\t<li>64</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 8\n",
       "\\item 64\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 8\n",
       "2. 64\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  8 64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>4</li>\n",
       "\t<li>16</li>\n",
       "\t<li>64</li>\n",
       "\t<li>256</li>\n",
       "\t<li>1024</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 4\n",
       "\\item 16\n",
       "\\item 64\n",
       "\\item 256\n",
       "\\item 1024\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 4\n",
       "2. 16\n",
       "3. 64\n",
       "4. 256\n",
       "5. 1024\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]    4   16   64  256 1024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.05</li>\n",
       "\t<li>0.01</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.05\n",
       "\\item 0.01\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.05\n",
       "2. 0.01\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.05 0.01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>nFeatures</th><th scope=col>nObs</th><th scope=col>nTimes</th><th scope=col>pValue</th><th scope=col>percSignificant</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>   4</td><td> 8  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>  16</td><td> 8  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>  64</td><td> 8  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td> 256</td><td> 8  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>1024</td><td> 8  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>   4</td><td>64  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>  16</td><td>64  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>  64</td><td>64  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td> 256</td><td>64  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>1024</td><td>64  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>   4</td><td> 8  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td>  16</td><td> 8  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td>  64</td><td> 8  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td> 256</td><td> 8  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td>1024</td><td> 8  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td>   4</td><td>64  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td>  16</td><td>64  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td>  64</td><td>64  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td> 256</td><td>64  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td>1024</td><td>64  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " nFeatures & nObs & nTimes & pValue & percSignificant\\\\\n",
       "\\hline\n",
       "\t    4 &  8   & 100  & 0.05 & NA  \\\\\n",
       "\t   16 &  8   & 100  & 0.05 & NA  \\\\\n",
       "\t   64 &  8   & 100  & 0.05 & NA  \\\\\n",
       "\t  256 &  8   & 100  & 0.05 & NA  \\\\\n",
       "\t 1024 &  8   & 100  & 0.05 & NA  \\\\\n",
       "\t    4 & 64   & 100  & 0.05 & NA  \\\\\n",
       "\t   16 & 64   & 100  & 0.05 & NA  \\\\\n",
       "\t   64 & 64   & 100  & 0.05 & NA  \\\\\n",
       "\t  256 & 64   & 100  & 0.05 & NA  \\\\\n",
       "\t 1024 & 64   & 100  & 0.05 & NA  \\\\\n",
       "\t    4 &  8   & 100  & 0.01 & NA  \\\\\n",
       "\t   16 &  8   & 100  & 0.01 & NA  \\\\\n",
       "\t   64 &  8   & 100  & 0.01 & NA  \\\\\n",
       "\t  256 &  8   & 100  & 0.01 & NA  \\\\\n",
       "\t 1024 &  8   & 100  & 0.01 & NA  \\\\\n",
       "\t    4 & 64   & 100  & 0.01 & NA  \\\\\n",
       "\t   16 & 64   & 100  & 0.01 & NA  \\\\\n",
       "\t   64 & 64   & 100  & 0.01 & NA  \\\\\n",
       "\t  256 & 64   & 100  & 0.01 & NA  \\\\\n",
       "\t 1024 & 64   & 100  & 0.01 & NA  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| nFeatures | nObs | nTimes | pValue | percSignificant |\n",
       "|---|---|---|---|---|\n",
       "|    4 |  8   | 100  | 0.05 | NA   |\n",
       "|   16 |  8   | 100  | 0.05 | NA   |\n",
       "|   64 |  8   | 100  | 0.05 | NA   |\n",
       "|  256 |  8   | 100  | 0.05 | NA   |\n",
       "| 1024 |  8   | 100  | 0.05 | NA   |\n",
       "|    4 | 64   | 100  | 0.05 | NA   |\n",
       "|   16 | 64   | 100  | 0.05 | NA   |\n",
       "|   64 | 64   | 100  | 0.05 | NA   |\n",
       "|  256 | 64   | 100  | 0.05 | NA   |\n",
       "| 1024 | 64   | 100  | 0.05 | NA   |\n",
       "|    4 |  8   | 100  | 0.01 | NA   |\n",
       "|   16 |  8   | 100  | 0.01 | NA   |\n",
       "|   64 |  8   | 100  | 0.01 | NA   |\n",
       "|  256 |  8   | 100  | 0.01 | NA   |\n",
       "| 1024 |  8   | 100  | 0.01 | NA   |\n",
       "|    4 | 64   | 100  | 0.01 | NA   |\n",
       "|   16 | 64   | 100  | 0.01 | NA   |\n",
       "|   64 | 64   | 100  | 0.01 | NA   |\n",
       "|  256 | 64   | 100  | 0.01 | NA   |\n",
       "| 1024 | 64   | 100  | 0.01 | NA   |\n",
       "\n"
      ],
      "text/plain": [
       "   nFeatures nObs nTimes pValue percSignificant\n",
       "1     4       8   100    0.05   NA             \n",
       "2    16       8   100    0.05   NA             \n",
       "3    64       8   100    0.05   NA             \n",
       "4   256       8   100    0.05   NA             \n",
       "5  1024       8   100    0.05   NA             \n",
       "6     4      64   100    0.05   NA             \n",
       "7    16      64   100    0.05   NA             \n",
       "8    64      64   100    0.05   NA             \n",
       "9   256      64   100    0.05   NA             \n",
       "10 1024      64   100    0.05   NA             \n",
       "11    4       8   100    0.01   NA             \n",
       "12   16       8   100    0.01   NA             \n",
       "13   64       8   100    0.01   NA             \n",
       "14  256       8   100    0.01   NA             \n",
       "15 1024       8   100    0.01   NA             \n",
       "16    4      64   100    0.01   NA             \n",
       "17   16      64   100    0.01   NA             \n",
       "18   64      64   100    0.01   NA             \n",
       "19  256      64   100    0.01   NA             \n",
       "20 1024      64   100    0.01   NA             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "20"
      ],
      "text/latex": [
       "20"
      ],
      "text/markdown": [
       "20"
      ],
      "text/plain": [
       "[1] 20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(thisManyObservations <- base2_seq[c(2, 5)])\n",
    "(thisManyFeatures <- base2_seq[seq(1, 9, 2)])\n",
    "(thesePValues <- c(0.05, 0.01))\n",
    "\n",
    "iterationTable <- expand.grid(nFeatures = thisManyFeatures, \n",
    "                              nObs = thisManyObservations, \n",
    "                              nTimes = 100, \n",
    "                              pValue = thesePValues, \n",
    "                              percSignificant = NA)\n",
    "iterationTable\n",
    "nrow(iterationTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>If you already have the iterationTable file, you can load it here instead of running the loop below and save time</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterationTable <- readRDS(\"DD&CP iterationTable.RDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>This next loop iterates over the above table and get our results. This may take a few minutes.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i in 1:nrow(iterationTable)){\n",
    "  sigProps <- mclapply(1:iterationTable$nTimes[i], function(x){\n",
    "    testData <- subsampleData(iterationTable$nObs[i], iterationTable$nFeatures[i])\n",
    "    pSig <- sapply(1:(ncol(testData) - 1), function(x){\n",
    "      groups <- testData$group\n",
    "      testData$group <- NULL\n",
    "      groups -> testData$group\n",
    "      dat <- testData[c(x, ncol(testData))]        \n",
    "      names(dat) <- c(\"var\", \"group\")\n",
    "      t.test(var ~ as.factor(group), dat)$p.value})\n",
    "    return(sum(pSig <= iterationTable$pValue[i]))\n",
    "  }) %>% unlist()\n",
    "  iterationTable$nSignificant[i] <- sum(sigProps)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>nFeatures</th><th scope=col>nObs</th><th scope=col>nTimes</th><th scope=col>pValue</th><th scope=col>percSignificant</th><th scope=col>nSignificant</th><th scope=col>pSignificant</th><th scope=col>nSigFeatures</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>   4      </td><td> 8        </td><td>100       </td><td>0.05      </td><td> 0.17     </td><td>  17      </td><td>0.04250000</td><td> 0.17     </td></tr>\n",
       "\t<tr><td>  16      </td><td> 8        </td><td>100       </td><td>0.05      </td><td> 0.85     </td><td>  85      </td><td>0.05312500</td><td> 0.85     </td></tr>\n",
       "\t<tr><td>  64      </td><td> 8        </td><td>100       </td><td>0.05      </td><td> 3.24     </td><td> 324      </td><td>0.05062500</td><td> 3.24     </td></tr>\n",
       "\t<tr><td> 256      </td><td> 8        </td><td>100       </td><td>0.05      </td><td>12.47     </td><td>1247      </td><td>0.04871094</td><td>12.47     </td></tr>\n",
       "\t<tr><td>1024      </td><td> 8        </td><td>100       </td><td>0.05      </td><td>51.84     </td><td>5184      </td><td>0.05062500</td><td>51.84     </td></tr>\n",
       "\t<tr><td>   4      </td><td>64        </td><td>100       </td><td>0.05      </td><td> 0.19     </td><td>  19      </td><td>0.04750000</td><td> 0.19     </td></tr>\n",
       "\t<tr><td>  16      </td><td>64        </td><td>100       </td><td>0.05      </td><td> 0.81     </td><td>  81      </td><td>0.05062500</td><td> 0.81     </td></tr>\n",
       "\t<tr><td>  64      </td><td>64        </td><td>100       </td><td>0.05      </td><td> 2.96     </td><td> 296      </td><td>0.04625000</td><td> 2.96     </td></tr>\n",
       "\t<tr><td> 256      </td><td>64        </td><td>100       </td><td>0.05      </td><td>12.90     </td><td>1290      </td><td>0.05039063</td><td>12.90     </td></tr>\n",
       "\t<tr><td>1024      </td><td>64        </td><td>100       </td><td>0.05      </td><td>51.57     </td><td>5157      </td><td>0.05036133</td><td>51.57     </td></tr>\n",
       "\t<tr><td>   4      </td><td> 8        </td><td>100       </td><td>0.01      </td><td> 0.05     </td><td>   5      </td><td>0.01250000</td><td> 0.05     </td></tr>\n",
       "\t<tr><td>  16      </td><td> 8        </td><td>100       </td><td>0.01      </td><td> 0.13     </td><td>  13      </td><td>0.00812500</td><td> 0.13     </td></tr>\n",
       "\t<tr><td>  64      </td><td> 8        </td><td>100       </td><td>0.01      </td><td> 0.74     </td><td>  74      </td><td>0.01156250</td><td> 0.74     </td></tr>\n",
       "\t<tr><td> 256      </td><td> 8        </td><td>100       </td><td>0.01      </td><td> 2.65     </td><td> 265      </td><td>0.01035156</td><td> 2.65     </td></tr>\n",
       "\t<tr><td>1024      </td><td> 8        </td><td>100       </td><td>0.01      </td><td>10.87     </td><td>1087      </td><td>0.01061523</td><td>10.87     </td></tr>\n",
       "\t<tr><td>   4      </td><td>64        </td><td>100       </td><td>0.01      </td><td> 0.04     </td><td>   4      </td><td>0.01000000</td><td> 0.04     </td></tr>\n",
       "\t<tr><td>  16      </td><td>64        </td><td>100       </td><td>0.01      </td><td> 0.11     </td><td>  11      </td><td>0.00687500</td><td> 0.11     </td></tr>\n",
       "\t<tr><td>  64      </td><td>64        </td><td>100       </td><td>0.01      </td><td> 0.68     </td><td>  68      </td><td>0.01062500</td><td> 0.68     </td></tr>\n",
       "\t<tr><td> 256      </td><td>64        </td><td>100       </td><td>0.01      </td><td> 2.61     </td><td> 261      </td><td>0.01019531</td><td> 2.61     </td></tr>\n",
       "\t<tr><td>1024      </td><td>64        </td><td>100       </td><td>0.01      </td><td>10.30     </td><td>1030      </td><td>0.01005859</td><td>10.30     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllll}\n",
       " nFeatures & nObs & nTimes & pValue & percSignificant & nSignificant & pSignificant & nSigFeatures\\\\\n",
       "\\hline\n",
       "\t    4       &  8         & 100        & 0.05       &  0.17      &   17       & 0.04250000 &  0.17     \\\\\n",
       "\t   16       &  8         & 100        & 0.05       &  0.85      &   85       & 0.05312500 &  0.85     \\\\\n",
       "\t   64       &  8         & 100        & 0.05       &  3.24      &  324       & 0.05062500 &  3.24     \\\\\n",
       "\t  256       &  8         & 100        & 0.05       & 12.47      & 1247       & 0.04871094 & 12.47     \\\\\n",
       "\t 1024       &  8         & 100        & 0.05       & 51.84      & 5184       & 0.05062500 & 51.84     \\\\\n",
       "\t    4       & 64         & 100        & 0.05       &  0.19      &   19       & 0.04750000 &  0.19     \\\\\n",
       "\t   16       & 64         & 100        & 0.05       &  0.81      &   81       & 0.05062500 &  0.81     \\\\\n",
       "\t   64       & 64         & 100        & 0.05       &  2.96      &  296       & 0.04625000 &  2.96     \\\\\n",
       "\t  256       & 64         & 100        & 0.05       & 12.90      & 1290       & 0.05039063 & 12.90     \\\\\n",
       "\t 1024       & 64         & 100        & 0.05       & 51.57      & 5157       & 0.05036133 & 51.57     \\\\\n",
       "\t    4       &  8         & 100        & 0.01       &  0.05      &    5       & 0.01250000 &  0.05     \\\\\n",
       "\t   16       &  8         & 100        & 0.01       &  0.13      &   13       & 0.00812500 &  0.13     \\\\\n",
       "\t   64       &  8         & 100        & 0.01       &  0.74      &   74       & 0.01156250 &  0.74     \\\\\n",
       "\t  256       &  8         & 100        & 0.01       &  2.65      &  265       & 0.01035156 &  2.65     \\\\\n",
       "\t 1024       &  8         & 100        & 0.01       & 10.87      & 1087       & 0.01061523 & 10.87     \\\\\n",
       "\t    4       & 64         & 100        & 0.01       &  0.04      &    4       & 0.01000000 &  0.04     \\\\\n",
       "\t   16       & 64         & 100        & 0.01       &  0.11      &   11       & 0.00687500 &  0.11     \\\\\n",
       "\t   64       & 64         & 100        & 0.01       &  0.68      &   68       & 0.01062500 &  0.68     \\\\\n",
       "\t  256       & 64         & 100        & 0.01       &  2.61      &  261       & 0.01019531 &  2.61     \\\\\n",
       "\t 1024       & 64         & 100        & 0.01       & 10.30      & 1030       & 0.01005859 & 10.30     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| nFeatures | nObs | nTimes | pValue | percSignificant | nSignificant | pSignificant | nSigFeatures |\n",
       "|---|---|---|---|---|---|---|---|\n",
       "|    4       |  8         | 100        | 0.05       |  0.17      |   17       | 0.04250000 |  0.17      |\n",
       "|   16       |  8         | 100        | 0.05       |  0.85      |   85       | 0.05312500 |  0.85      |\n",
       "|   64       |  8         | 100        | 0.05       |  3.24      |  324       | 0.05062500 |  3.24      |\n",
       "|  256       |  8         | 100        | 0.05       | 12.47      | 1247       | 0.04871094 | 12.47      |\n",
       "| 1024       |  8         | 100        | 0.05       | 51.84      | 5184       | 0.05062500 | 51.84      |\n",
       "|    4       | 64         | 100        | 0.05       |  0.19      |   19       | 0.04750000 |  0.19      |\n",
       "|   16       | 64         | 100        | 0.05       |  0.81      |   81       | 0.05062500 |  0.81      |\n",
       "|   64       | 64         | 100        | 0.05       |  2.96      |  296       | 0.04625000 |  2.96      |\n",
       "|  256       | 64         | 100        | 0.05       | 12.90      | 1290       | 0.05039063 | 12.90      |\n",
       "| 1024       | 64         | 100        | 0.05       | 51.57      | 5157       | 0.05036133 | 51.57      |\n",
       "|    4       |  8         | 100        | 0.01       |  0.05      |    5       | 0.01250000 |  0.05      |\n",
       "|   16       |  8         | 100        | 0.01       |  0.13      |   13       | 0.00812500 |  0.13      |\n",
       "|   64       |  8         | 100        | 0.01       |  0.74      |   74       | 0.01156250 |  0.74      |\n",
       "|  256       |  8         | 100        | 0.01       |  2.65      |  265       | 0.01035156 |  2.65      |\n",
       "| 1024       |  8         | 100        | 0.01       | 10.87      | 1087       | 0.01061523 | 10.87      |\n",
       "|    4       | 64         | 100        | 0.01       |  0.04      |    4       | 0.01000000 |  0.04      |\n",
       "|   16       | 64         | 100        | 0.01       |  0.11      |   11       | 0.00687500 |  0.11      |\n",
       "|   64       | 64         | 100        | 0.01       |  0.68      |   68       | 0.01062500 |  0.68      |\n",
       "|  256       | 64         | 100        | 0.01       |  2.61      |  261       | 0.01019531 |  2.61      |\n",
       "| 1024       | 64         | 100        | 0.01       | 10.30      | 1030       | 0.01005859 | 10.30      |\n",
       "\n"
      ],
      "text/plain": [
       "   nFeatures nObs nTimes pValue percSignificant nSignificant pSignificant\n",
       "1     4       8   100    0.05    0.17             17         0.04250000  \n",
       "2    16       8   100    0.05    0.85             85         0.05312500  \n",
       "3    64       8   100    0.05    3.24            324         0.05062500  \n",
       "4   256       8   100    0.05   12.47           1247         0.04871094  \n",
       "5  1024       8   100    0.05   51.84           5184         0.05062500  \n",
       "6     4      64   100    0.05    0.19             19         0.04750000  \n",
       "7    16      64   100    0.05    0.81             81         0.05062500  \n",
       "8    64      64   100    0.05    2.96            296         0.04625000  \n",
       "9   256      64   100    0.05   12.90           1290         0.05039063  \n",
       "10 1024      64   100    0.05   51.57           5157         0.05036133  \n",
       "11    4       8   100    0.01    0.05              5         0.01250000  \n",
       "12   16       8   100    0.01    0.13             13         0.00812500  \n",
       "13   64       8   100    0.01    0.74             74         0.01156250  \n",
       "14  256       8   100    0.01    2.65            265         0.01035156  \n",
       "15 1024       8   100    0.01   10.87           1087         0.01061523  \n",
       "16    4      64   100    0.01    0.04              4         0.01000000  \n",
       "17   16      64   100    0.01    0.11             11         0.00687500  \n",
       "18   64      64   100    0.01    0.68             68         0.01062500  \n",
       "19  256      64   100    0.01    2.61            261         0.01019531  \n",
       "20 1024      64   100    0.01   10.30           1030         0.01005859  \n",
       "   nSigFeatures\n",
       "1   0.17       \n",
       "2   0.85       \n",
       "3   3.24       \n",
       "4  12.47       \n",
       "5  51.84       \n",
       "6   0.19       \n",
       "7   0.81       \n",
       "8   2.96       \n",
       "9  12.90       \n",
       "10 51.57       \n",
       "11  0.05       \n",
       "12  0.13       \n",
       "13  0.74       \n",
       "14  2.65       \n",
       "15 10.87       \n",
       "16  0.04       \n",
       "17  0.11       \n",
       "18  0.68       \n",
       "19  2.61       \n",
       "20 10.30       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterationTable$percSignificant <- iterationTable$nSignificant / iterationTable$nTimes\n",
    "iterationTable <- iterationTable %>% mutate(pSignificant = percSignificant / nFeatures)\n",
    "iterationTable <- iterationTable %>% mutate(nSigFeatures = pSignificant * nFeatures)\n",
    "iterationTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>You can save the data by uncommenting the below line of code</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveRDS(iterationTable, \"DD&CP iterationTable.RDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We can see from these many iterations that randomly generated data can yield a number of significantly different features that is directly proportional to the level of significance we measure at.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAH0CAMAAAD8CC+4AAAAHlBMVEUAAAAAv8QaGhozMzNN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\ngG6o/wHyeaZokL8d7wAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterationTable %>% \n",
    "    ggplot(aes(as.factor(nObs), pSignificant, col = as.factor(pValue))) + \n",
    "    geom_boxplot() + \n",
    "    geom_jitter(width = 0.1) +\n",
    "    labs(x = \"Number of observations\", y = \"Proportion of significantly different features\") +\n",
    "    scale_colour_discrete(name = \"P-value\") +\n",
    "    theme(legend.position = \"top\") +\n",
    "    facet_wrap(~pValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>What is most concerning about patterns (significant features) in noise, from a machine learning perspective, is that as the number of features increases, the number of significant features can increase proportionally with the alpha level of significance of our significance test. The number of observations in this example does not impact the result.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAH0CAMAAAD8CC+4AAAAHlBMVEUAAAAAv8QaGhozMzNN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\nooib39vNgQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterationTable %>% \n",
    "    ggplot(aes(nFeatures, nSigFeatures, col = as.factor(nObs))) + \n",
    "    geom_line() + \n",
    "    geom_point() +\n",
    "    labs(x = \"Number of features\", y = \"Number of significantly different features\") +\n",
    "    scale_colour_discrete(name = \"Number of observations\") +\n",
    "    theme(legend.position = \"top\") +\n",
    "    facet_wrap(~pValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>In other words, <u>as the number of features increase</u>, machine learning algorithms are given <u>more opportunity to learn patterns that arise from chance/noise</u>.</h2>\n",
    "\n",
    "<h4>Because of this statistical likelihood we can be fooled into thinking that we can distinguish between different classes. The fact that we are being fooled will not necessarily be evident from accuracy scores or other metrics such as Cohen's Kappa. This is where, wherever possible, we must use our specialist knowledge to scrutinise the decision-making behaviour of the algorithm.</h4> \n",
    "\n",
    "<h4>In some instances it may not be able to peer into the black box of the algorithm, so it is doubly important that caution must be used when interpreting model results where large numbers of features are present in the data.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We should assume that some features will return as significant <b>by chance</b></li>\n",
    "    <li>Some algorithms use significance tests or make decisions similarly, therefore:\n",
    "        <ul>\n",
    "        <li>It is likely that the proportion of potential chance significant features approximately equals the alpha level of significance</li>\n",
    "        <li>It is likely prudent to check that there are many (a threshold) more significant features than the alpha level of significance</li>\n",
    "        </ul>\n",
    "    <li>We should be mindful when using feature selection algorithms</li>\n",
    "    <li>We should use subject-specific knowedge where possible</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes to self:\n",
    "Should we use something like the Bonferroni correction and set alpha = 1/n (n = # of wavenumbers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
