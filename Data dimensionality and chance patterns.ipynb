{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data dimensionality and chance patterns that may return as significant</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore the potential for patterns to arise by chance in data as we increase the number of features in a dataset. We create what is essentially a null model by randomly generating a dataset and performing some significance tests (a <i>t</i>-test in this case) on the features to assess what chance can achieve in terms of significant differences between randomly assigned classes.\n",
    "\n",
    "We find that the proportion of significant features over many tests tends towards the alpha level of significance we are using; i.e., ~5% (1/20 or 0.05) of features will randomly contain a \"signal\" at the typical alpha level of significance ($p$ = 0.05), or, more elegantly, the proportion of significant features ($P$) will approximately equal the alpha level of significance ($\\alpha$):\n",
    "\n",
    "$$P \\approx \\alpha$$\n",
    "\n",
    "The number of likely significant features ($F_{s}$) of the total number of features ($F$) is approximately:\n",
    "\n",
    "$$F_{s} \\approx F \\cdot \\alpha$$\n",
    "\n",
    "If we have a dataset containing 20 features, 1 would be expected to test as significant by chance with an $\\alpha$ of 0.05: \n",
    "\n",
    "$$F_{s} \\approx F \\cdot \\alpha \\approx 20 \\cdot 0.05 \\approx 1$$\n",
    "\n",
    "However, spectral datasets typically contain hundreds or thousands of features. For instance, a typical fingerprint region of an FTIR dataset contains ~400 features. Using an alpha level of significance of approximately 0.05, we might expect that:\n",
    "\n",
    "$$F_{s} \\approx F \\cdot \\alpha \\approx 400 \\cdot 0.05 \\approx 20$$\n",
    "\n",
    "We would expect around 20 features to test as significant by chance in a dataset generated at random.\n",
    "\n",
    "Obviously we are not dealing with random datasets and may not be using algorithms that use stistical tests, but given this potential, it is important to consider whether machine learning algorithms are finding genuine, useful patterns in our data. As we increase the dimensionality of our data (the number of features), we increase the potential for algorithms to find chance patterns. Ultimately we may be fooled into thinking that we have found genuine, predictable differences between our samples.\n",
    "\n",
    "This is demostrated in the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Loading libraries, assigning functions and setting plotting options</h4>\n",
    "call.do() is simply a switch of arguments from do.call() that allows it to be used easily in piped code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'dplyr' was built under R version 3.6.3\"\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'tidyr' was built under R version 3.6.3\""
     ]
    }
   ],
   "source": [
    "library(parallel)\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(tidyr)\n",
    "call.do <- function(args, what){do.call(what, args)}\n",
    "options(repr.plot.width = 5, repr.plot.height = 5, repr.plot.res = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Creating a set of data randomly drawn from a uniform distribution. It has 8192 observations and 1024 features. (Many features are typical of spectra.)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>8192</li>\n",
       "\t<li>1024</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 8192\n",
       "\\item 1024\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 8192\n",
       "2. 1024\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 8192 1024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nObservation <- 2^13\n",
    "nFeature <- 2^10\n",
    "set.seed(1138)\n",
    "randomData <- lapply(1:nObservation, function(i){runif(nFeature)}) %>% call.do(rbind)\n",
    "dim(randomData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Randomly assigning observations to one of two groups, a and b, and adding that to the randomly generated data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nGroup <- 2 # this script will only run with two groups as we as using a t-test as a test of significance\n",
    "classSize <- nObservation / nGroup\n",
    "randomGroup <- rep(letters[1:nGroup], each = classSize)\n",
    "randomData <- data.frame(randomData) %>% cbind(data.frame(group = randomGroup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Establishing a sequence of numbers to iterate. It is base 2 where each number is double the previous one in the sequence, so in a small sequence we can explore quite different sizes of datasets. We will use it to iterate over various numbers of randomly selected observations and features</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>4</li>\n",
       "\t<li>8</li>\n",
       "\t<li>16</li>\n",
       "\t<li>32</li>\n",
       "\t<li>64</li>\n",
       "\t<li>128</li>\n",
       "\t<li>256</li>\n",
       "\t<li>512</li>\n",
       "\t<li>1024</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 4\n",
       "\\item 8\n",
       "\\item 16\n",
       "\\item 32\n",
       "\\item 64\n",
       "\\item 128\n",
       "\\item 256\n",
       "\\item 512\n",
       "\\item 1024\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 4\n",
       "2. 8\n",
       "3. 16\n",
       "4. 32\n",
       "5. 64\n",
       "6. 128\n",
       "7. 256\n",
       "8. 512\n",
       "9. 1024\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]    4    8   16   32   64  128  256  512 1024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAH0CAMAAAD8CC+4AAAABlBMVEUAAAD///+l2Z/dAAAA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\nQEYHMjqQ0YGMDmR0IKMDGR3I6ED/AEuHxanc7hDJAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(base2_seq <- 2^c(2:10))\n",
    "plot(base2_seq, type = \"b\", las = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Defining a couple of functions to randomly subset data in terms of observations and features. This will happen many times to demonstrate that the tendency of a significant result is a function of the alpha level of significance of the statistical test.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampleObs <- function(n){\n",
    "    randomData %>% \n",
    "    split(randomData$group) %>% \n",
    "    lapply(function(x){x[sample(nrow(x), n), ]})\n",
    "}\n",
    "\n",
    "subsampleData <- function(nObs, nFeatures){\n",
    "    subSample <- subsampleObs(nObs) %>% call.do(rbind)\n",
    "    groups <- subSample$group\n",
    "    subSample <- subSample %>% \n",
    "    group_by(group) %>% \n",
    "    sample(nFeatures) %>% \n",
    "    ungroup()\n",
    "    subSample$group <- groups\n",
    "    return(subSample)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>X724</th><th scope=col>X964</th><th scope=col>X680</th><th scope=col>X672</th><th scope=col>group</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.91325417</td><td>0.91356999</td><td>0.24879003</td><td>0.7744510 </td><td>a         </td></tr>\n",
       "\t<tr><td>0.31943702</td><td>0.80814421</td><td>0.87576071</td><td>0.0266545 </td><td>a         </td></tr>\n",
       "\t<tr><td>0.83799467</td><td>0.11916436</td><td>0.92487120</td><td>0.8930405 </td><td>a         </td></tr>\n",
       "\t<tr><td>0.50749201</td><td>0.30984967</td><td>0.80390678</td><td>0.9311738 </td><td>a         </td></tr>\n",
       "\t<tr><td>0.83215577</td><td>0.05320815</td><td>0.26137463</td><td>0.3662314 </td><td>a         </td></tr>\n",
       "\t<tr><td>0.04496013</td><td>0.62565044</td><td>0.59033348</td><td>0.5541183 </td><td>a         </td></tr>\n",
       "\t<tr><td>0.78673155</td><td>0.69675860</td><td>0.04321533</td><td>0.2788866 </td><td>a         </td></tr>\n",
       "\t<tr><td>0.33946182</td><td>0.48922367</td><td>0.22549794</td><td>0.2260483 </td><td>a         </td></tr>\n",
       "\t<tr><td>0.70547272</td><td>0.02456188</td><td>0.10982789</td><td>0.5146272 </td><td>b         </td></tr>\n",
       "\t<tr><td>0.47156608</td><td>0.56404949</td><td>0.89785499</td><td>0.9215434 </td><td>b         </td></tr>\n",
       "\t<tr><td>0.13870818</td><td>0.41419604</td><td>0.21875003</td><td>0.3168169 </td><td>b         </td></tr>\n",
       "\t<tr><td>0.65864660</td><td>0.43511121</td><td>0.38920819</td><td>0.6209096 </td><td>b         </td></tr>\n",
       "\t<tr><td>0.83594948</td><td>0.23494593</td><td>0.43840957</td><td>0.4357076 </td><td>b         </td></tr>\n",
       "\t<tr><td>0.51862051</td><td>0.08643279</td><td>0.66030008</td><td>0.2444577 </td><td>b         </td></tr>\n",
       "\t<tr><td>0.30556725</td><td>0.54003579</td><td>0.20947761</td><td>0.9351345 </td><td>b         </td></tr>\n",
       "\t<tr><td>0.95120256</td><td>0.41245939</td><td>0.14003876</td><td>0.3687491 </td><td>b         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " X724 & X964 & X680 & X672 & group\\\\\n",
       "\\hline\n",
       "\t 0.91325417 & 0.91356999 & 0.24879003 & 0.7744510  & a         \\\\\n",
       "\t 0.31943702 & 0.80814421 & 0.87576071 & 0.0266545  & a         \\\\\n",
       "\t 0.83799467 & 0.11916436 & 0.92487120 & 0.8930405  & a         \\\\\n",
       "\t 0.50749201 & 0.30984967 & 0.80390678 & 0.9311738  & a         \\\\\n",
       "\t 0.83215577 & 0.05320815 & 0.26137463 & 0.3662314  & a         \\\\\n",
       "\t 0.04496013 & 0.62565044 & 0.59033348 & 0.5541183  & a         \\\\\n",
       "\t 0.78673155 & 0.69675860 & 0.04321533 & 0.2788866  & a         \\\\\n",
       "\t 0.33946182 & 0.48922367 & 0.22549794 & 0.2260483  & a         \\\\\n",
       "\t 0.70547272 & 0.02456188 & 0.10982789 & 0.5146272  & b         \\\\\n",
       "\t 0.47156608 & 0.56404949 & 0.89785499 & 0.9215434  & b         \\\\\n",
       "\t 0.13870818 & 0.41419604 & 0.21875003 & 0.3168169  & b         \\\\\n",
       "\t 0.65864660 & 0.43511121 & 0.38920819 & 0.6209096  & b         \\\\\n",
       "\t 0.83594948 & 0.23494593 & 0.43840957 & 0.4357076  & b         \\\\\n",
       "\t 0.51862051 & 0.08643279 & 0.66030008 & 0.2444577  & b         \\\\\n",
       "\t 0.30556725 & 0.54003579 & 0.20947761 & 0.9351345  & b         \\\\\n",
       "\t 0.95120256 & 0.41245939 & 0.14003876 & 0.3687491  & b         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| X724 | X964 | X680 | X672 | group |\n",
       "|---|---|---|---|---|\n",
       "| 0.91325417 | 0.91356999 | 0.24879003 | 0.7744510  | a          |\n",
       "| 0.31943702 | 0.80814421 | 0.87576071 | 0.0266545  | a          |\n",
       "| 0.83799467 | 0.11916436 | 0.92487120 | 0.8930405  | a          |\n",
       "| 0.50749201 | 0.30984967 | 0.80390678 | 0.9311738  | a          |\n",
       "| 0.83215577 | 0.05320815 | 0.26137463 | 0.3662314  | a          |\n",
       "| 0.04496013 | 0.62565044 | 0.59033348 | 0.5541183  | a          |\n",
       "| 0.78673155 | 0.69675860 | 0.04321533 | 0.2788866  | a          |\n",
       "| 0.33946182 | 0.48922367 | 0.22549794 | 0.2260483  | a          |\n",
       "| 0.70547272 | 0.02456188 | 0.10982789 | 0.5146272  | b          |\n",
       "| 0.47156608 | 0.56404949 | 0.89785499 | 0.9215434  | b          |\n",
       "| 0.13870818 | 0.41419604 | 0.21875003 | 0.3168169  | b          |\n",
       "| 0.65864660 | 0.43511121 | 0.38920819 | 0.6209096  | b          |\n",
       "| 0.83594948 | 0.23494593 | 0.43840957 | 0.4357076  | b          |\n",
       "| 0.51862051 | 0.08643279 | 0.66030008 | 0.2444577  | b          |\n",
       "| 0.30556725 | 0.54003579 | 0.20947761 | 0.9351345  | b          |\n",
       "| 0.95120256 | 0.41245939 | 0.14003876 | 0.3687491  | b          |\n",
       "\n"
      ],
      "text/plain": [
       "   X724       X964       X680       X672      group\n",
       "1  0.91325417 0.91356999 0.24879003 0.7744510 a    \n",
       "2  0.31943702 0.80814421 0.87576071 0.0266545 a    \n",
       "3  0.83799467 0.11916436 0.92487120 0.8930405 a    \n",
       "4  0.50749201 0.30984967 0.80390678 0.9311738 a    \n",
       "5  0.83215577 0.05320815 0.26137463 0.3662314 a    \n",
       "6  0.04496013 0.62565044 0.59033348 0.5541183 a    \n",
       "7  0.78673155 0.69675860 0.04321533 0.2788866 a    \n",
       "8  0.33946182 0.48922367 0.22549794 0.2260483 a    \n",
       "9  0.70547272 0.02456188 0.10982789 0.5146272 b    \n",
       "10 0.47156608 0.56404949 0.89785499 0.9215434 b    \n",
       "11 0.13870818 0.41419604 0.21875003 0.3168169 b    \n",
       "12 0.65864660 0.43511121 0.38920819 0.6209096 b    \n",
       "13 0.83594948 0.23494593 0.43840957 0.4357076 b    \n",
       "14 0.51862051 0.08643279 0.66030008 0.2444577 b    \n",
       "15 0.30556725 0.54003579 0.20947761 0.9351345 b    \n",
       "16 0.95120256 0.41245939 0.14003876 0.3687491 b    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAH0CAMAAAD8CC+4AAAAFVBMVEUAAAAaGhozMzNNTU3Z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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is an example of the output it produces given its input parameters: \n",
    "# 8 randomly selected observations of each class and 4 randomly selected features\n",
    "# For each iteration, this subsampling will be performed 100 times and a t-test applied\n",
    "(exampleData <- subsampleData(nObs = 8, nFeatures = 4))\n",
    "exampleData %>% \n",
    "    gather(\"pseudoWavenumber\", \"pseudoAbsorbance\", -group) %>%\n",
    "    ggplot(aes(group, pseudoAbsorbance)) +\n",
    "    scale_y_continuous(limits = c(0, 1)) +\n",
    "    geom_boxplot(outlier.shape = NA) +\n",
    "    geom_jitter(size = 3, width = 0.1) +\n",
    "    facet_wrap(~pseudoWavenumber, nrow = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Generating a table of values to iterate over (# of observations, # of features and p-value we are taking as significant and we will store the percentage of significantly different features in the column</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"thisManyObservations\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>8</li>\n",
       "\t<li>64</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 8\n",
       "\\item 64\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 8\n",
       "2. 64\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  8 64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"thisManyFeatures\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>4</li>\n",
       "\t<li>16</li>\n",
       "\t<li>64</li>\n",
       "\t<li>256</li>\n",
       "\t<li>1024</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 4\n",
       "\\item 16\n",
       "\\item 64\n",
       "\\item 256\n",
       "\\item 1024\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 4\n",
       "2. 16\n",
       "3. 64\n",
       "4. 256\n",
       "5. 1024\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]    4   16   64  256 1024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"thesePValues\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.05</li>\n",
       "\t<li>0.01</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.05\n",
       "\\item 0.01\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.05\n",
       "2. 0.01\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.05 0.01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>nFeatures</th><th scope=col>nObs</th><th scope=col>nTimes</th><th scope=col>pValue</th><th scope=col>percSignificant</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>   4</td><td> 8  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>  16</td><td> 8  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>  64</td><td> 8  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td> 256</td><td> 8  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>1024</td><td> 8  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>   4</td><td>64  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>  16</td><td>64  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>  64</td><td>64  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td> 256</td><td>64  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>1024</td><td>64  </td><td>100 </td><td>0.05</td><td>NA  </td></tr>\n",
       "\t<tr><td>   4</td><td> 8  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td>  16</td><td> 8  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td>  64</td><td> 8  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td> 256</td><td> 8  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td>1024</td><td> 8  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td>   4</td><td>64  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td>  16</td><td>64  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td>  64</td><td>64  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td> 256</td><td>64  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "\t<tr><td>1024</td><td>64  </td><td>100 </td><td>0.01</td><td>NA  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " nFeatures & nObs & nTimes & pValue & percSignificant\\\\\n",
       "\\hline\n",
       "\t    4 &  8   & 100  & 0.05 & NA  \\\\\n",
       "\t   16 &  8   & 100  & 0.05 & NA  \\\\\n",
       "\t   64 &  8   & 100  & 0.05 & NA  \\\\\n",
       "\t  256 &  8   & 100  & 0.05 & NA  \\\\\n",
       "\t 1024 &  8   & 100  & 0.05 & NA  \\\\\n",
       "\t    4 & 64   & 100  & 0.05 & NA  \\\\\n",
       "\t   16 & 64   & 100  & 0.05 & NA  \\\\\n",
       "\t   64 & 64   & 100  & 0.05 & NA  \\\\\n",
       "\t  256 & 64   & 100  & 0.05 & NA  \\\\\n",
       "\t 1024 & 64   & 100  & 0.05 & NA  \\\\\n",
       "\t    4 &  8   & 100  & 0.01 & NA  \\\\\n",
       "\t   16 &  8   & 100  & 0.01 & NA  \\\\\n",
       "\t   64 &  8   & 100  & 0.01 & NA  \\\\\n",
       "\t  256 &  8   & 100  & 0.01 & NA  \\\\\n",
       "\t 1024 &  8   & 100  & 0.01 & NA  \\\\\n",
       "\t    4 & 64   & 100  & 0.01 & NA  \\\\\n",
       "\t   16 & 64   & 100  & 0.01 & NA  \\\\\n",
       "\t   64 & 64   & 100  & 0.01 & NA  \\\\\n",
       "\t  256 & 64   & 100  & 0.01 & NA  \\\\\n",
       "\t 1024 & 64   & 100  & 0.01 & NA  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| nFeatures | nObs | nTimes | pValue | percSignificant |\n",
       "|---|---|---|---|---|\n",
       "|    4 |  8   | 100  | 0.05 | NA   |\n",
       "|   16 |  8   | 100  | 0.05 | NA   |\n",
       "|   64 |  8   | 100  | 0.05 | NA   |\n",
       "|  256 |  8   | 100  | 0.05 | NA   |\n",
       "| 1024 |  8   | 100  | 0.05 | NA   |\n",
       "|    4 | 64   | 100  | 0.05 | NA   |\n",
       "|   16 | 64   | 100  | 0.05 | NA   |\n",
       "|   64 | 64   | 100  | 0.05 | NA   |\n",
       "|  256 | 64   | 100  | 0.05 | NA   |\n",
       "| 1024 | 64   | 100  | 0.05 | NA   |\n",
       "|    4 |  8   | 100  | 0.01 | NA   |\n",
       "|   16 |  8   | 100  | 0.01 | NA   |\n",
       "|   64 |  8   | 100  | 0.01 | NA   |\n",
       "|  256 |  8   | 100  | 0.01 | NA   |\n",
       "| 1024 |  8   | 100  | 0.01 | NA   |\n",
       "|    4 | 64   | 100  | 0.01 | NA   |\n",
       "|   16 | 64   | 100  | 0.01 | NA   |\n",
       "|   64 | 64   | 100  | 0.01 | NA   |\n",
       "|  256 | 64   | 100  | 0.01 | NA   |\n",
       "| 1024 | 64   | 100  | 0.01 | NA   |\n",
       "\n"
      ],
      "text/plain": [
       "   nFeatures nObs nTimes pValue percSignificant\n",
       "1     4       8   100    0.05   NA             \n",
       "2    16       8   100    0.05   NA             \n",
       "3    64       8   100    0.05   NA             \n",
       "4   256       8   100    0.05   NA             \n",
       "5  1024       8   100    0.05   NA             \n",
       "6     4      64   100    0.05   NA             \n",
       "7    16      64   100    0.05   NA             \n",
       "8    64      64   100    0.05   NA             \n",
       "9   256      64   100    0.05   NA             \n",
       "10 1024      64   100    0.05   NA             \n",
       "11    4       8   100    0.01   NA             \n",
       "12   16       8   100    0.01   NA             \n",
       "13   64       8   100    0.01   NA             \n",
       "14  256       8   100    0.01   NA             \n",
       "15 1024       8   100    0.01   NA             \n",
       "16    4      64   100    0.01   NA             \n",
       "17   16      64   100    0.01   NA             \n",
       "18   64      64   100    0.01   NA             \n",
       "19  256      64   100    0.01   NA             \n",
       "20 1024      64   100    0.01   NA             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "20"
      ],
      "text/latex": [
       "20"
      ],
      "text/markdown": [
       "20"
      ],
      "text/plain": [
       "[1] 20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"thisManyObservations\")\n",
    "(thisManyObservations <- base2_seq[c(2, 5)])\n",
    "print(\"thisManyFeatures\")\n",
    "(thisManyFeatures <- base2_seq[seq(1, 9, 2)])\n",
    "print(\"thesePValues\")\n",
    "(thesePValues <- c(0.05, 0.01))\n",
    "\n",
    "iterationTable <- expand.grid(nFeatures = thisManyFeatures, \n",
    "                              nObs = thisManyObservations, \n",
    "                              nTimes = 100, \n",
    "                              pValue = thesePValues, \n",
    "                              percSignificant = NA)\n",
    "iterationTable\n",
    "nrow(iterationTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>If you already have the iterationTable file, you can load it here instead of running the loop below and save time</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterationTable <- readRDS(\"DD&CP iterationTable.RDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>This next loop iterates over the above table and get our results. This may take a few minutes.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i in 1:nrow(iterationTable)){\n",
    "  sigProps <- mclapply(1:iterationTable$nTimes[i], function(x){\n",
    "    testData <- subsampleData(iterationTable$nObs[i], iterationTable$nFeatures[i])\n",
    "    pSig <- sapply(1:(ncol(testData) - 1), function(x){\n",
    "      groups <- testData$group\n",
    "      testData$group <- NULL\n",
    "      groups -> testData$group\n",
    "      dat <- testData[c(x, ncol(testData))]        \n",
    "      names(dat) <- c(\"var\", \"group\")\n",
    "      t.test(var ~ as.factor(group), dat)$p.value})\n",
    "    return(sum(pSig <= iterationTable$pValue[i]))\n",
    "  }) %>% unlist()\n",
    "  iterationTable$nSignificant[i] <- sum(sigProps)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Adding some columns for average percentage of significant features (percSignificant), the proportion of significant features (pSignificant) and the number of significant features (nSigFeatures)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>nFeatures</th><th scope=col>nObs</th><th scope=col>nTimes</th><th scope=col>pValue</th><th scope=col>percSignificant</th><th scope=col>nSignificant</th><th scope=col>pSignificant</th><th scope=col>nSigFeatures</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>   4      </td><td> 8        </td><td>100       </td><td>0.05      </td><td> 0.15     </td><td>  15      </td><td>0.03750000</td><td> 0.15     </td></tr>\n",
       "\t<tr><td>  16      </td><td> 8        </td><td>100       </td><td>0.05      </td><td> 0.82     </td><td>  82      </td><td>0.05125000</td><td> 0.82     </td></tr>\n",
       "\t<tr><td>  64      </td><td> 8        </td><td>100       </td><td>0.05      </td><td> 2.96     </td><td> 296      </td><td>0.04625000</td><td> 2.96     </td></tr>\n",
       "\t<tr><td> 256      </td><td> 8        </td><td>100       </td><td>0.05      </td><td>12.40     </td><td>1240      </td><td>0.04843750</td><td>12.40     </td></tr>\n",
       "\t<tr><td>1024      </td><td> 8        </td><td>100       </td><td>0.05      </td><td>51.67     </td><td>5167      </td><td>0.05045898</td><td>51.67     </td></tr>\n",
       "\t<tr><td>   4      </td><td>64        </td><td>100       </td><td>0.05      </td><td> 0.19     </td><td>  19      </td><td>0.04750000</td><td> 0.19     </td></tr>\n",
       "\t<tr><td>  16      </td><td>64        </td><td>100       </td><td>0.05      </td><td> 0.81     </td><td>  81      </td><td>0.05062500</td><td> 0.81     </td></tr>\n",
       "\t<tr><td>  64      </td><td>64        </td><td>100       </td><td>0.05      </td><td> 2.96     </td><td> 296      </td><td>0.04625000</td><td> 2.96     </td></tr>\n",
       "\t<tr><td> 256      </td><td>64        </td><td>100       </td><td>0.05      </td><td>12.90     </td><td>1290      </td><td>0.05039063</td><td>12.90     </td></tr>\n",
       "\t<tr><td>1024      </td><td>64        </td><td>100       </td><td>0.05      </td><td>51.57     </td><td>5157      </td><td>0.05036133</td><td>51.57     </td></tr>\n",
       "\t<tr><td>   4      </td><td> 8        </td><td>100       </td><td>0.01      </td><td> 0.05     </td><td>   5      </td><td>0.01250000</td><td> 0.05     </td></tr>\n",
       "\t<tr><td>  16      </td><td> 8        </td><td>100       </td><td>0.01      </td><td> 0.13     </td><td>  13      </td><td>0.00812500</td><td> 0.13     </td></tr>\n",
       "\t<tr><td>  64      </td><td> 8        </td><td>100       </td><td>0.01      </td><td> 0.74     </td><td>  74      </td><td>0.01156250</td><td> 0.74     </td></tr>\n",
       "\t<tr><td> 256      </td><td> 8        </td><td>100       </td><td>0.01      </td><td> 2.65     </td><td> 265      </td><td>0.01035156</td><td> 2.65     </td></tr>\n",
       "\t<tr><td>1024      </td><td> 8        </td><td>100       </td><td>0.01      </td><td>10.87     </td><td>1087      </td><td>0.01061523</td><td>10.87     </td></tr>\n",
       "\t<tr><td>   4      </td><td>64        </td><td>100       </td><td>0.01      </td><td> 0.04     </td><td>   4      </td><td>0.01000000</td><td> 0.04     </td></tr>\n",
       "\t<tr><td>  16      </td><td>64        </td><td>100       </td><td>0.01      </td><td> 0.11     </td><td>  11      </td><td>0.00687500</td><td> 0.11     </td></tr>\n",
       "\t<tr><td>  64      </td><td>64        </td><td>100       </td><td>0.01      </td><td> 0.68     </td><td>  68      </td><td>0.01062500</td><td> 0.68     </td></tr>\n",
       "\t<tr><td> 256      </td><td>64        </td><td>100       </td><td>0.01      </td><td> 2.61     </td><td> 261      </td><td>0.01019531</td><td> 2.61     </td></tr>\n",
       "\t<tr><td>1024      </td><td>64        </td><td>100       </td><td>0.01      </td><td>10.30     </td><td>1030      </td><td>0.01005859</td><td>10.30     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllll}\n",
       " nFeatures & nObs & nTimes & pValue & percSignificant & nSignificant & pSignificant & nSigFeatures\\\\\n",
       "\\hline\n",
       "\t    4       &  8         & 100        & 0.05       &  0.15      &   15       & 0.03750000 &  0.15     \\\\\n",
       "\t   16       &  8         & 100        & 0.05       &  0.82      &   82       & 0.05125000 &  0.82     \\\\\n",
       "\t   64       &  8         & 100        & 0.05       &  2.96      &  296       & 0.04625000 &  2.96     \\\\\n",
       "\t  256       &  8         & 100        & 0.05       & 12.40      & 1240       & 0.04843750 & 12.40     \\\\\n",
       "\t 1024       &  8         & 100        & 0.05       & 51.67      & 5167       & 0.05045898 & 51.67     \\\\\n",
       "\t    4       & 64         & 100        & 0.05       &  0.19      &   19       & 0.04750000 &  0.19     \\\\\n",
       "\t   16       & 64         & 100        & 0.05       &  0.81      &   81       & 0.05062500 &  0.81     \\\\\n",
       "\t   64       & 64         & 100        & 0.05       &  2.96      &  296       & 0.04625000 &  2.96     \\\\\n",
       "\t  256       & 64         & 100        & 0.05       & 12.90      & 1290       & 0.05039063 & 12.90     \\\\\n",
       "\t 1024       & 64         & 100        & 0.05       & 51.57      & 5157       & 0.05036133 & 51.57     \\\\\n",
       "\t    4       &  8         & 100        & 0.01       &  0.05      &    5       & 0.01250000 &  0.05     \\\\\n",
       "\t   16       &  8         & 100        & 0.01       &  0.13      &   13       & 0.00812500 &  0.13     \\\\\n",
       "\t   64       &  8         & 100        & 0.01       &  0.74      &   74       & 0.01156250 &  0.74     \\\\\n",
       "\t  256       &  8         & 100        & 0.01       &  2.65      &  265       & 0.01035156 &  2.65     \\\\\n",
       "\t 1024       &  8         & 100        & 0.01       & 10.87      & 1087       & 0.01061523 & 10.87     \\\\\n",
       "\t    4       & 64         & 100        & 0.01       &  0.04      &    4       & 0.01000000 &  0.04     \\\\\n",
       "\t   16       & 64         & 100        & 0.01       &  0.11      &   11       & 0.00687500 &  0.11     \\\\\n",
       "\t   64       & 64         & 100        & 0.01       &  0.68      &   68       & 0.01062500 &  0.68     \\\\\n",
       "\t  256       & 64         & 100        & 0.01       &  2.61      &  261       & 0.01019531 &  2.61     \\\\\n",
       "\t 1024       & 64         & 100        & 0.01       & 10.30      & 1030       & 0.01005859 & 10.30     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| nFeatures | nObs | nTimes | pValue | percSignificant | nSignificant | pSignificant | nSigFeatures |\n",
       "|---|---|---|---|---|---|---|---|\n",
       "|    4       |  8         | 100        | 0.05       |  0.15      |   15       | 0.03750000 |  0.15      |\n",
       "|   16       |  8         | 100        | 0.05       |  0.82      |   82       | 0.05125000 |  0.82      |\n",
       "|   64       |  8         | 100        | 0.05       |  2.96      |  296       | 0.04625000 |  2.96      |\n",
       "|  256       |  8         | 100        | 0.05       | 12.40      | 1240       | 0.04843750 | 12.40      |\n",
       "| 1024       |  8         | 100        | 0.05       | 51.67      | 5167       | 0.05045898 | 51.67      |\n",
       "|    4       | 64         | 100        | 0.05       |  0.19      |   19       | 0.04750000 |  0.19      |\n",
       "|   16       | 64         | 100        | 0.05       |  0.81      |   81       | 0.05062500 |  0.81      |\n",
       "|   64       | 64         | 100        | 0.05       |  2.96      |  296       | 0.04625000 |  2.96      |\n",
       "|  256       | 64         | 100        | 0.05       | 12.90      | 1290       | 0.05039063 | 12.90      |\n",
       "| 1024       | 64         | 100        | 0.05       | 51.57      | 5157       | 0.05036133 | 51.57      |\n",
       "|    4       |  8         | 100        | 0.01       |  0.05      |    5       | 0.01250000 |  0.05      |\n",
       "|   16       |  8         | 100        | 0.01       |  0.13      |   13       | 0.00812500 |  0.13      |\n",
       "|   64       |  8         | 100        | 0.01       |  0.74      |   74       | 0.01156250 |  0.74      |\n",
       "|  256       |  8         | 100        | 0.01       |  2.65      |  265       | 0.01035156 |  2.65      |\n",
       "| 1024       |  8         | 100        | 0.01       | 10.87      | 1087       | 0.01061523 | 10.87      |\n",
       "|    4       | 64         | 100        | 0.01       |  0.04      |    4       | 0.01000000 |  0.04      |\n",
       "|   16       | 64         | 100        | 0.01       |  0.11      |   11       | 0.00687500 |  0.11      |\n",
       "|   64       | 64         | 100        | 0.01       |  0.68      |   68       | 0.01062500 |  0.68      |\n",
       "|  256       | 64         | 100        | 0.01       |  2.61      |  261       | 0.01019531 |  2.61      |\n",
       "| 1024       | 64         | 100        | 0.01       | 10.30      | 1030       | 0.01005859 | 10.30      |\n",
       "\n"
      ],
      "text/plain": [
       "   nFeatures nObs nTimes pValue percSignificant nSignificant pSignificant\n",
       "1     4       8   100    0.05    0.15             15         0.03750000  \n",
       "2    16       8   100    0.05    0.82             82         0.05125000  \n",
       "3    64       8   100    0.05    2.96            296         0.04625000  \n",
       "4   256       8   100    0.05   12.40           1240         0.04843750  \n",
       "5  1024       8   100    0.05   51.67           5167         0.05045898  \n",
       "6     4      64   100    0.05    0.19             19         0.04750000  \n",
       "7    16      64   100    0.05    0.81             81         0.05062500  \n",
       "8    64      64   100    0.05    2.96            296         0.04625000  \n",
       "9   256      64   100    0.05   12.90           1290         0.05039063  \n",
       "10 1024      64   100    0.05   51.57           5157         0.05036133  \n",
       "11    4       8   100    0.01    0.05              5         0.01250000  \n",
       "12   16       8   100    0.01    0.13             13         0.00812500  \n",
       "13   64       8   100    0.01    0.74             74         0.01156250  \n",
       "14  256       8   100    0.01    2.65            265         0.01035156  \n",
       "15 1024       8   100    0.01   10.87           1087         0.01061523  \n",
       "16    4      64   100    0.01    0.04              4         0.01000000  \n",
       "17   16      64   100    0.01    0.11             11         0.00687500  \n",
       "18   64      64   100    0.01    0.68             68         0.01062500  \n",
       "19  256      64   100    0.01    2.61            261         0.01019531  \n",
       "20 1024      64   100    0.01   10.30           1030         0.01005859  \n",
       "   nSigFeatures\n",
       "1   0.15       \n",
       "2   0.82       \n",
       "3   2.96       \n",
       "4  12.40       \n",
       "5  51.67       \n",
       "6   0.19       \n",
       "7   0.81       \n",
       "8   2.96       \n",
       "9  12.90       \n",
       "10 51.57       \n",
       "11  0.05       \n",
       "12  0.13       \n",
       "13  0.74       \n",
       "14  2.65       \n",
       "15 10.87       \n",
       "16  0.04       \n",
       "17  0.11       \n",
       "18  0.68       \n",
       "19  2.61       \n",
       "20 10.30       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterationTable <- iterationTable %>% mutate(percSignificant = nSignificant / nTimes,\n",
    "                                            pSignificant = percSignificant / nFeatures,\n",
    "                                            nSigFeatures = pSignificant * nFeatures)\n",
    "iterationTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>You can save the data by uncommenting the below line of code</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveRDS(iterationTable, \"DD&CP iterationTable.RDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We can see from these many iterations that randomly generated data can yield a number of significantly different features that is directly proportional to the level of significance we measure at.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAH0CAMAAAD8CC+4AAAAHlBMVEUAAAAAv8QaGhozMzNN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\nDQV0QwHdUP8DLMqsR8sMibIAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterationTable %>% \n",
    "    ggplot(aes(as.factor(nObs), pSignificant, col = as.factor(pValue))) + \n",
    "    geom_boxplot() + \n",
    "    geom_jitter(width = 0.1) +\n",
    "    labs(x = \"Number of observations\", y = \"Proportion of significantly different features\") +\n",
    "    scale_colour_discrete(name = \"P-value\") +\n",
    "    theme(legend.position = \"top\") +\n",
    "    facet_wrap(~pValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>What is most concerning about patterns (significant features) in noise, from a machine learning perspective, is that as the number of features increases, the number of significant features can increase proportionally with the alpha level of significance of our significance test. The number of observations in this example does not impact the result.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAH0CAMAAAD8CC+4AAAAHlBMVEUAAAAAv8QaGhozMzNN\nTU3Z2dnr6+vy8vL4dm3///+TLSLTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAUEElEQVR4nO2d\ni7akKAxF9Xb1Hev/f3i6yhfKGxII5py1Zm6LJoTaBUKpcXpD6jT1DgBqL0BXKEBXKEBXKEBX\nKEBXKEBXKEBXKEBXKEBXKEBXKEBXKEBXKEBXKEAv0H8eURzdQoBeIEBXKEBXKEBXKEBXKEBX\nKEBXKEAn1HT549sdcnA9JLdxqccDOqE2ZsXQ7wfkNC7n2CjGyfz2pR/dCoYs6Gs4I0FflsXG\nOF38GUfP8xw4elIO/fz77QZrX5j2PjGd25vhftRl2zKwyt/TxXYKmJq6MDeon+1wQ59nk7p5\n9BZ11aeXLmHQ98/rgL5yMDduO7d/nzved+u3/VU6jr1+zayKz+2LNthXBaDPti5Hb18vrcO7\n9dlvf93sDDs39Ei5ucP3/XDFGevpvnO6p6cftWuFvn3QYejTNiSbdmG4W782DQ/oZ5mrYteY\ny3JO933DGCQO+vqFj/f0txF7FPo5XBuGxrQxNMQ4sP/nkRmPE3ro6Mk1e+CRFugOw8no7CHo\n9ofEAt1RD5fkQT/PcRZ0F5jLAfdtD8npetg7MLxf4O+KYcQ6PUNG27cV1K2n31deh+GtILpk\nM6rba7ov2c5Asof3mqNbSBT0UQToCgXoCgXoCgXoCgXo0HACdIUCdIUCdIUCdIUCdIUCdIUC\ndIUCdIUCdIUCdIUCdIUCdIUCdIUCdIUCdIUCdIUCdIUCdIUCdIUCdIUCdIUCdIXKhP73uVLU\nREDfpaiJgL5LURMBfZeiJgL6LkVNBPRdipoI6LsUNRHQdylqIqDvUtREQN+lqIkCof/8/Nz+\n9eM/mE4Nm2i18Ocs4ZRc6D9/d8r7v5p8IA2hO1rIX+lHA0H/eVpPd7SwjQaC/rjh3YbeZnQH\ndEudezrO6cqg//0L6IDOJEC/qyN0DO/b2vXn+NfjoNstVD+R6yVFTQT0XYqaCOi7FDUR0Hcp\naiKg71LUREDfpaiJgL5LURMzof8eMv55UW45v0GiI6uJrQPgdwTogA7ogA7oHQLgdwTogA7o\ngA7oHQLgdwTogA7ogA7oHQLgdwTogA7ogA7oHQLgdwTogA7oj4W+LMuxCeg6oC/zPO/UAV0J\n9PmjfRPQAR3QAR3QnwJ9OZkDurXjodBncxPQVUBfAD2045HQL8wB3drxSOjzdTMF+uuf1j+A\nzh8Ag6OlBPrx/516w4ABvd7RlXkS9Nf5B9D5A6B3tJRA30b3HfqfP39C4wIkTMvs2RHr6S/0\n9FYBkDua7+Up0N+A3jIAakcLoEd3PA76bJWnQMfw3jQAYkdLGfR9gY51epMAiB3NdnkSdFuN\nAiY10Al9dpQD+rOhL4CesuNZ0GdXOaA/GvoC6Ek7HgV9dpYD+pOhz+5yQH8w9AXQ9UGfPeWA\n/lzos6cc0K0dgA7o40KfPeWAbu94CvQF0NMdPQX67Cn/BXR7x0Ogz57y7yagAzqgPwP67Clf\nNwH9idDvTzfcDgP0J0K/3+h+2wT0B0KfPeX7phP6Z2sKfQ8YA2YzAPRj0wV9Wv8LUGcMmM1A\nD/Q7c0CP7Rgf+v3RtWTo070M0FsFUO3IYg7osR3DQ7eZJ07kIswBnTGAWkel0KPiCpjTQAl0\nB3Mi6JBY+R5Gd+g+vIcn7+jpjAHUOXJ1dEzkYjvGhu5kDuixHYAO6GNBdzMH9NgOrdA/E7kJ\nF1w6BVDhyMMc6/TYjoGhz4kGTujxbwB9wIBeXw7opY7GhW6lDvMZuDlHqZMHDOj15XXQpyk2\nkyMPGNCry+3UYT6D5L4N6K0CKCx3pA7zGQA6oG8bGN6Hg+5KHeYzCPR0/DjTJ4CyciLo+Bm2\nTwBF5c7UYT4DQH8EdHfqMJ8BoAP6toELLmNB96QO8xkk9GpAbxtAfvn5YvQ0A3dPj34R6AJu\nZ/Bg6JkGgD4+dG/qMF+5A/o04ccZddBxaXUs6P7UYb7ydM6A3iiAzPJjFpfuyA0dw/tA0PMd\nOaFPscfTAZ0xgLzyUOowX3kAOmbvfQLIK2eEjveytQsgqzyYOsxX7oN+Hd+vb1UGdNYAcsrN\nWVy6Iyd0K7sUoDcMIKc8nC/OV+6GftPrjfeni9S8VJnnQH+jp3MGkFEeyRfnK3dDvyYlMN6o\nDOj8AaSXx1KH+cqd0G+z99frO28H9DYBJJffZnHpjlKgv9HTWwaQXB7NF+crz4COdXqbAFLL\n7Y5ODP2u2oB7GDwOerEjJ3QkJRgAekK+OF+5G3pUlQF3MXgWdMfgXgE9cn0N0LkDSCtPyRfn\nKwf0MaE7O3rF8I575EaAXuPIAR09XT70tNRhvvJ0zoDeKICEcvfgDujFjoaAXucI0AeE7uvo\ngF7qaATolY6c0PFYk2jo3hyBgF7qSDz0eWGAjnW6cOjVjkI9PaDSgAG92uDfLI4FelyFAQfK\nAT2xPJAYsg46hnex0EM5AjG8lzqSDf27RAd0akfCoVM4AvShoK+/xTGd0wFdKHQSR+6ejomc\nTOjbj+48PT2qgoAj5YAeL98vtAA6tSPJ0IkcuaHjBbsSoUcTQ9bO3vGwQ7cAfOXnVfRO0KH2\nqnwY3SH09GPr3kQhPd24XQbDO7UjsdCpHAUmcniWrVcA7vKUxJBYspU6kgn9ci8k1/AO6N0C\ncJYDOmvNIqFfb3pmOqcDujDoVI7WTWdPxwUXWdATE0NiIlfqSCD0+xMtgE7tSC10XHCRBN16\ndI1r9o5f5LoFYJUnJ4YE9FJH4qCnJ4YE9FJH0qBn5AgE9FJHWqHjgosc6K4EBDzQo8quhyxg\n9pqlQadyZG5a0JFHThL0rMSQgF7qSBT0vByBFcM7khIohI6eLga6J40UhndqR4AO6D2h5yaG\nxDm91JEc6Nk5AnFOL3WkFXpc2fWQBcxesxjo/nxxPOd0DO/KoKOny4AeyBfXBPr2Qja8l61J\nAKs+J/SGw7s1e99evYg3MLYJYFVb6MfWpQjQmwXwVTBfHAt0RxboF96f3lT0D6M7FIb+wgt2\n2wXwUThfXI+eDuisAfxGU4cxndOtZTqgNwvgtxP0qzB7bxpAPF9cC+hYpzcNQAh0W9n1kAXM\nXnN/6NF8cYBO7ag79Hi+OECndgTogN48gIR8cSzQgw+3ADprAPNvJ+hx7Nn1kAXMXnNv6An5\n4tiGdzzL1iWApNRhnOf0APfywAA9uKMv9JW4j3p5YIAe2pGWL475nA7oTQMwZ3FVjsLlTuiY\nvfeCnpYvjvOcDuiNA7h19IbQ7XvkAL1RAKn54tDTqR31g56cL47nnB79IpQHBui+HffBHdAJ\nDMRDT84XxwAd5/Qu0O2O3qWnB1QeGKB7dmTki8NEjtpRJ+g5+eJ4oGN4bw3dMbhjeCcwkA09\nJ18coFM76gLd2dEBvd5ANPSsfHFM53RAbwo9M3UYT0/HRK4pdPfg3rqnR1UeGKDbys0XB+jU\njtpDn98ioEeHd4hQTRIQuOR4nQfuhm0TQH7qMK4lG97h0iqAf7M4QGczkAo9P18cF/TI+F4e\nGKBf9VmuiYC+Usc5vUUAJfnisGSjdtQWelG+OECndtQU+vpbnAzodsZIQGcJoCxfHNtELky9\nPDBAN7T96A7obAYSoZfliwN0akcNoe9X1wCdzUAc9OOKqgjoeKtyG+il+eJ4oEdVHhig7zpv\nnQB0NgNh0I3bZfpDx9uaFEJHT28CvSZfHE9PB3R+6BX54gCd2lEb6JcbYPtDx6PKDaBfb3oW\nAB09XSX0uPoFzF5zC+iV+eJ4zukY3pmh16UOQ0+ndtQAem2+OECndsQP3Xp0TQZ0DO/6oGP2\nzgm9Pl8coFM74odenTqsCXS8jI8wAIJ8cUzn9CvzN167SRaAKwGBCOiOdTqgEwUgFrpDJvQ/\nf/5EjoZ8mt+9HkZ3KAL99UZPJwmAJF8c6/B+DvCAThMATb44xomc8Yj6ywQP6MUBeNJIiYB+\n3vc+ncwBvT6AgaC/XusKHev0ygCo8sU16em2+gXMXjMndKLUYUzn9G0W553T9wuYvWY+6L6O\nLgR6VP0CZq+ZDbo3RyCg8xkA+rFpQcftUjzQvYO7BOjo6YAO6DQBBBJDCoGOPHLU0EM5AmVA\nR8ZIQAf06oq+J3Tx0JEQmDQAQO9k0BF6ODGkDOjrJA4TOaoAIjkChUCPql/A7DUDOqATBBDL\nEQjofAaAfmwCOjf0aGJIQOcz6AQ9niNQAHQkGgJ0QK8LICFHoADoyC5FCX3+HQM6ejop9Hhi\nSBnQ4+oXMHvNpNCTcgQKgY587/qg480ORNDTcgQCOp9Be+jmLK7KUWI5M3QoRaIeRncIPf3Y\nujexOIDUHIEyejomchqhR9UvYPaayaAn5wgEdD6DxtBvs7hyR+nlgF7qiA56amJIQOczaAvd\n7uiA3sGgNfTkxJAyoOOCSzX0nByBgM5n0BK6Y3CXDj1OvV/A7DUTQXc8lywbOm6iqII+z86O\nLhx6XP0CZq+5Gvr8UU5iSEDnM2gKvSKA8vLKc3ownxigB3cMCh1PrdZA//UwB/QOBq2gLx9V\nBFBeDuiljgAd0LMDmAeFjpsoaqD/ephLhx5Vv4DZa66EHsgXB+jtDZpA//wWJ6Ul3nI3dAzv\n5dD9+eJkQ8fdsKXQg6nDAL29QciRMfcCdCXQzVVWDfRwvjjZ0M9XdAF6TgCR1GGCoU/qkhIs\ni3roSeoXMHXN/2i/L9dIyqHHUocNBn17G9/z3su2rOdg82qoWuj3N60+87Wb+5BOBD2aL042\n9Fu3fz3xXavGIm2eaaDHUocNBf3+gt3x35/+OZFv+kf7+H+VpD+M7lAO9PfYPf3axy2D972J\naQEk5IuT3dMt6g+CfiduGeiFflunPwX62cnN+9gooKfki5MN/ZnDu5u4ZVAIPSF12IjQh16n\nr7/BfGTfq0oAPSlf3GDQLfULuMRgWfbrIM67kymgp6QOkw39WQ8w7qP6O+1+9BLo16eYxoT+\noAsux4ncR9xyVAD99ozqmNDj6hdwjsGVeKKjEuhpqcMAnd1gJ378wsoGPTVfnGzo4w/vBvFc\nRwXQE1OHyYYe7/z9Ak4wcBFPd5QN3XoWfWTog94j5yae7igXenq+OEDnMfh08nfScty7Ixt6\ncuowQGcw2Ib1ysfDM6E7Eo2MCT06jxMI/SRe6Ugr9Lj6BezccenjTaG7MgoBOnXAjh23UR3Q\nY+XDQ187ecp6nAN6VuowwdBHetjBJl5fcw50J/MRob8DRaKgO4nX16wZenDy3h+69WAKWc0Z\n0N3Mx4UeOcf3C/grTx8nqVkt9HA37wz9O6wXrMfJoXuYjwk9irwj9J04X80Z0HkC4HfkgJ6y\neusTsEGcr+Zk6L6OPiJ0qUu2K3G+mlOhz9lZpARDT1LzgO/E+WpOht5vtaoC+qeT16/HaaHP\n+anDAD15h5s4X82p0HssXIgcCYf++Q2Gaj1OCn0uSB02OvQm2oj3DsMpmVFlSlxPP0b11v3D\naqLLbmYMgN+RUOjGeVwi9LkodRigB3Ysi/nqG5HQOQPgdyQO+pd4jgFZzftWHPpcljoM0J07\nLOIxA7Kaja0E6KwB8DsSBN1FPGhAVvN1Kwp99u2gCYDfkRjobuIBg27Qi/PFAfpFS/3t6uXl\n2dCZA+B3JAD6QnK7enl5JnT2y3z8jnpDX6huVy8vB/Sm0BfC29XLy3OgXyYegJ5fP+3t6mSO\nQtCvL0wG9OT614+N/HZ1MkeATg/9mmc7P+ByA0A/NrtArwi43ADn9GOzLXRvP5fzUQV7eosA\n+B21hL5eHmd6MIXMUQD6LXBAj9RzrM7IWt4eeqM7M/kdNYFurscBvd5APvTblZRxoVtnJUB3\n12NfOxsWuj0TAXRHPZ8L5OQBlxsA+rHJBd1zS8Sw0Ns9bcHviAX64iNOEHC5QRX0nO+vlJZ4\nyymhr0lgNuLsLW8InejnpCdC/7A++/iDoPt+RAR065WlTAGXGwD6sUkGfam7BCXmowL0izzv\nZfueytenUh4JnewS0YjQnW9gXFGvK/Kq645iPqo7dC9ztdDneZ7dnwqglwbA76gEuvH+9J15\n3HpYPbh9lT293be03KDinN40AH5HldB/vXfCPAe6GFZkjmqh92s5oBcbADqgh6kP/P70dEeA\n7lO/gNlrBnRAF8SKzBGgAzqgAzqgdwiA3xGgAzqgAzqgdwiA3xGgAzqgAzqgdwiA31Eh9FN/\n4odUHd/foHsAbBUAutwA2CoAdLkBsFUg8R0uELMAXaEAXaEAXaEAXaEKoRu3zSUd+Fr/Jphd\nj4wbvE6DpBpet7B8tSS3kL2J9C0sg27eIJt04CvZ7HJkWj0vyy5w7Ms4LFBLcgubNJG4hczQ\n32elaWbXI9OhJ9bwesc/ktRQr0ezNpG4hY2gbyNT3Ox6ZEY3SK2BDTpjE6lbyA89r4FxHE6T\ndAMO6NxNpG5hG+hvZ90Bi2zoyTVwQU8O4HZkMvTkCgRAzwv4dmTWJ9INOncTyVvIDf1lHs83\n9lEOfqa/FLE3kbyFzOv0V+aa9H5kioHLLnY44Tqdv4nkLcQvcgoF6AoF6AoF6AoF6AoF6AoF\n6AoF6AoF6Ar1NOjT5Y9vd8jBdP37RD2taRurYujT7W+W8Sh6TEM2TWuLAD2kxzRk0wn9/Dt9\nuv/3f/vGd3Pf3gz3o6bglun1fsAwGinWFG20Tejf7cncuO3c/n3ueB9Obl+fu9fbAaNooFCT\n5IC+/XUWToadF/rNscfrQBot3phO4mHo0zZkm3YW9Ok2vluO9gOGGtyfCX09scd7+jsG/X3d\nCI3oQ2EfKdYUcUB3ntMv1taGbA0UapLW9hyDsQXddcK/HGBsX2Ab0G2vZ81DaKBQkzSdf7YV\n1g3Pfcl2GF4LzIMOT/vS7/hyYMkGjSJAVyhAVyhAVyhAVyhAVyhAVyhAVyhAVyhAV6j/AVSt\nfg04gy2pAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterationTable %>% \n",
    "    ggplot(aes(nFeatures, nSigFeatures, col = as.factor(nObs))) + \n",
    "    geom_line() + \n",
    "    geom_point() +\n",
    "    labs(x = \"Number of features\", y = \"Number of significantly different features\") +\n",
    "    scale_colour_discrete(name = \"Number of observations\") +\n",
    "    theme(legend.position = \"top\") +\n",
    "    facet_wrap(~pValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>In other words, <u>as the number of features increase</u>, machine learning algorithms are given <u>more opportunity to learn patterns that arise from chance/noise</u>.</h2>\n",
    "\n",
    "<h4>Because of this statistical likelihood we can be fooled into thinking that we can distinguish between different classes. The fact that we are being fooled will not necessarily be evident from accuracy scores or other metrics such as Cohen's Kappa. This is where, wherever possible, we must use our specialist knowledge to scrutinise the decision-making behaviour of the algorithm.</h4> \n",
    "\n",
    "<h4>In some instances it may not be able to peer into the black box of the algorithm, so it is doubly important that caution must be used when interpreting model results where large numbers of features are present in the data.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We should assume that some features will return as significant <b>by chance</b></li>\n",
    "    <li>Some algorithms use significance tests or make decisions similarly, therefore:\n",
    "        <ul>\n",
    "        <li>It is likely that the proportion of potential chance significant features approximately equals the alpha level of significance</li>\n",
    "        <li>It is likely prudent to check that there are many (a threshold) more significant features than the alpha level of significance</li>\n",
    "        </ul>\n",
    "    <li>We should be mindful when using feature selection algorithms</li>\n",
    "    <li>We should use subject-specific knowedge where possible</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes to self:\n",
    "Should we use something like the Bonferroni correction and set alpha = 1/n (n = # of wavenumbers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
